from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, File, UploadFile, HTTPException
from contextlib import asynccontextmanager
from PIL import Image
import numpy as np
import tensorflow as tf
import tf_keras  # <--- CRITICAL FIX: Use this library for legacy/compatibility models
import os
import io
import json
import logging

# --- 1. SILENCE LOGS (Must be done before importing TensorFlow) ---
# 0=All, 1=Filter INFO, 2=Filter WARNING, 3=Filter ERROR
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TF_XLA_FLAGS"] = "--tf_xla_auto_jit=0"
os.environ["TF_CUDNN_USE_AUTOTUNE"] = "0"

# Set Python logging to show only errors/critical info
logging.basicConfig(level=logging.ERROR)

# --- 2. IMPORTS ---

# --- 3. CONFIGURATION ---
MODEL_PATH = 'god_recognizer_nightly.keras'
LABELS_PATH = 'class_names_nightly.json'
IMG_SIZE = (300, 300)
CONFIDENCE_THRESHOLD = 60.0

# Global resources dictionary
resources = {}

# --- 4. LIFESPAN (Model Loading) ---


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("--- [STARTUP] Loading AI Model & Labels... ---")
    try:
        # FIX: Use tf_keras.models.load_model instead of tf.keras.models.load_model
        resources['model'] = tf_keras.models.load_model(MODEL_PATH)

        with open(LABELS_PATH, 'r') as f:
            resources['class_names'] = json.load(f)

        print("✅ [SUCCESS] Model loaded successfully!")
    except Exception as e:
        # Print a short error message instead of the full stack trace dump
        print(f"❌ [CRITICAL ERROR] Failed to load model: {e}")
        resources['model'] = None

    yield  # Application runs here

    print("--- [SHUTDOWN] Cleaning up resources... ---")
    resources.clear()

# --- 5. FASTAPI APP INIT ---
app = FastAPI(title="God Recognizer API", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 6. ENDPOINTS ---


@app.get("/")
def home():
    status = "online" if resources.get('model') else "model_error"
    return {"status": status, "message": "God Recognizer API is running"}


@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    # 1. Fail fast if model isn't ready
    if resources.get('model') is None:
        raise HTTPException(
            status_code=500, detail="Model is not loaded on server.")

    # 2. Validate file type
    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="File must be an image")

    try:
        # 3. Process Image
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))

        if image.mode != "RGB":
            image = image.convert("RGB")

        image = image.resize(IMG_SIZE)

        # Use simple numpy array conversion to avoid TF/Keras version conflicts
        img_array = np.array(image)
        img_array = tf.expand_dims(img_array, 0)

        # 4. Predict
        # verbose=0 suppresses the progress bar logs
        predictions = resources['model'].predict(img_array, verbose=0)
        scores = tf.nn.softmax(predictions[0])

        top_k = 3
        top_indices = np.argsort(scores)[::-1][:top_k]

        results = []
        for i in top_indices:
            results.append({
                "god": resources['class_names'][i],
                "confidence": float(scores[i]) * 100
            })

        return {
            "prediction": results[0]["god"],
            "confidence": results[0]["confidence"],
            "top_3": results
        }

    except Exception as e:
        # Log only short error to console
        print(f"⚠️ Prediction Error: {str(e)}")
        raise HTTPException(
            status_code=500, detail="Internal prediction error")

if __name__ == "__main__":
    import uvicorn
    # log_level="error" suppresses Uvicorn's INFO logs
    uvicorn.run(app, host="0.0.0.0", port=8005, log_level="error")
